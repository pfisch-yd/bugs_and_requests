# ğŸš€ FreePort Optimization Guide

## âš ï¸ Problema de Custo Identificado

A funÃ§Ã£o original `filter_non_null_columns` tem **ALTO CUSTO** porque executa **N queries separadas** (uma por coluna):

```python
# âŒ PROBLEMA: Loop com N queries
for column in all_columns:  # 50 colunas = 50 queries!
    df = spark.sql(f"""
        SELECT COUNT({column})
        FROM {prod_schema}.{demo_name}{table_suffix}
        WHERE {column} IS NOT NULL
    """)
    # Cada query faz FULL TABLE SCAN da tabela inteira!
```

### ğŸ’° Impacto Real:

**Exemplo:** Tabela com 50 colunas e 1 bilhÃ£o de linhas

| Abordagem | Queries | Scans | Custo Relativo |
|-----------|---------|-------|----------------|
| âŒ Original (N queries) | 50 | 50 full scans | **100% (baseline)** |
| âœ… Otimizada (1 query) | 1 | 1 full scan | **2% (98% economia!)** |
| âœ…âœ… Sampling (1 query + sample) | 1 | 1% dos dados | **0.02% (99.98% economia!)** |

---

## âœ… SoluÃ§Ã£o 1: Single Query Optimization (RECOMENDADO)

Ao invÃ©s de N queries, executa **1 query Ãºnica** que verifica todas as colunas:

```python
def filter_non_null_columns_optimized(prod_schema, demo_name, table_suffix, exclude_columns=None):
    """
    âœ… OPTIMIZED: Uses SINGLE query with COUNT(*) FILTER
    """
    # Get column names
    all_columns = get_columns(...)

    # Build single query checking ALL columns at once
    count_expressions = []
    for column in all_columns:
        count_expr = f"COUNT(*) FILTER (WHERE {column} IS NOT NULL) as {column}_count"
        count_expressions.append(count_expr)

    # ONE query to rule them all!
    count_query = f"""
        SELECT
            {','.join(count_expressions)}
        FROM {prod_schema}.{demo_name}{table_suffix}
    """

    counts_df = spark.sql(count_query)
    counts_row = counts_df.collect()[0]

    # Filter columns with count > 0
    accepted_columns = [col for i, col in enumerate(all_columns) if counts_row[i] > 0]

    return accepted_columns
```

### ğŸ“Š ComparaÃ§Ã£o:

```
âŒ ORIGINAL (50 colunas):
Query 1: SELECT COUNT(col1) FROM table WHERE col1 IS NOT NULL  â†’ Full scan
Query 2: SELECT COUNT(col2) FROM table WHERE col2 IS NOT NULL  â†’ Full scan
...
Query 50: SELECT COUNT(col50) FROM table WHERE col50 IS NOT NULL â†’ Full scan
TOTAL: 50 full table scans

âœ… OTIMIZADA (50 colunas):
Query 1: SELECT
    COUNT(*) FILTER (WHERE col1 IS NOT NULL) as col1_count,
    COUNT(*) FILTER (WHERE col2 IS NOT NULL) as col2_count,
    ...
    COUNT(*) FILTER (WHERE col50 IS NOT NULL) as col50_count
FROM table
TOTAL: 1 full table scan

ECONOMIA: 98% de reduÃ§Ã£o de custo!
```

---

## âœ…âœ… SoluÃ§Ã£o 2: Sampling Optimization (MÃXIMA ECONOMIA)

Se ainda for muito caro, usa **amostragem** da tabela:

```python
def filter_non_null_columns_sampling(
    prod_schema, demo_name, table_suffix,
    exclude_columns=None,
    sample_fraction=0.01  # 1% dos dados
):
    """
    âœ…âœ… MOST OPTIMIZED: Uses sampling to reduce cost even further
    """
    count_query = f"""
        SELECT
            {','.join(count_expressions)}
        FROM {prod_schema}.{demo_name}{table_suffix}
        TABLESAMPLE ({sample_fraction * 100} PERCENT)  -- Only scans 1% of data!
    """
    # ... rest of logic
```

### Trade-off:
- âœ… **Vantagem**: Custo **99.98% menor**
- âš ï¸ **Desvantagem**: Pode perder colunas com poucos valores nÃ£o-nulos

---

## ğŸ¯ Qual Usar?

### Use **Single Query Optimization** quando:
- âœ… Precisa de **100% precisÃ£o**
- âœ… Tabelas atÃ© **alguns bilhÃµes de linhas**
- âœ… **98% de economia jÃ¡ Ã© suficiente**
- âœ… **RECOMENDADO para maioria dos casos**

### Use **Sampling** quando:
- âœ… Tabelas **muito grandes** (trilhÃµes de linhas)
- âœ… Pode tolerar perder colunas com **valores raros**
- âœ… Precisa de **mÃ¡xima economia** de custo
- âš ï¸ Entende o trade-off de precisÃ£o

---

## ğŸ“ Como Usar as VersÃµes Otimizadas

### OpÃ§Ã£o 1: Single Query (padrÃ£o, recomendado)

```python
from freeport_module_template_optimized import freeport_module

# Usa otimizaÃ§Ã£o de single query automaticamente
deliverable = freeport_module(
    sandbox_schema,
    prod_schema,
    demo_name,
    "pro_insights"  # Qualquer mÃ³dulo DYNAMIC
)
```

### OpÃ§Ã£o 2: Sampling (mÃ¡xima economia)

```python
# Usa apenas 1% dos dados para verificar colunas
deliverable = freeport_module(
    sandbox_schema,
    prod_schema,
    demo_name,
    "pro_insights",
    use_sampling=True,      # âœ… Ativa sampling
    sample_fraction=0.01    # 1% dos dados
)
```

### OpÃ§Ã£o 3: Batch com otimizaÃ§Ã£o

```python
from freeport_module_template_optimized import run_all_modules

# Roda todos os mÃ³dulos com single query optimization
results = run_all_modules(sandbox_schema, prod_schema, demo_name)

# Ou com sampling para mÃ¡xima economia
results = run_all_modules(
    sandbox_schema,
    prod_schema,
    demo_name,
    use_sampling=True,
    sample_fraction=0.01
)
```

---

## ğŸ” ComparaÃ§Ã£o TÃ©cnica Detalhada

### CÃ³digo Original (âŒ Caro)

```python
def filter_non_null_columns(prod_schema, demo_name, table_suffix, exclude_columns=None):
    df = spark.sql(f"SELECT *{exclude_clause} FROM {prod_schema}.{demo_name}{table_suffix}")
    all_columns = df.columns
    accepted_columns = []

    # âŒ LOOP: Executa N queries
    for i in range(0, len(all_columns)):
        df = spark.sql(f"""
            with filtered as (
                SELECT {all_columns[i]} as important_column
                FROM {prod_schema}.{demo_name}{table_suffix}
                where {all_columns[i]} is not null
            )
            select count(important_column) as count_rows from filtered
        """)

        if df.collect()[0][0] > 0:
            accepted_columns.append(all_columns[i])

    return accepted_columns
```

**Problemas:**
1. Loop executa `N` queries separadas
2. Cada query faz full table scan
3. Overhead de mÃºltiplas conexÃµes/execuÃ§Ãµes
4. **Custo proporcional ao nÃºmero de colunas**

### CÃ³digo Otimizado (âœ… Barato)

```python
def filter_non_null_columns_optimized(prod_schema, demo_name, table_suffix, exclude_columns=None):
    # Get columns (LIMIT 1 = quase zero custo)
    df = spark.sql(f"SELECT *{exclude_clause} FROM {prod_schema}.{demo_name}{table_suffix} LIMIT 1")
    all_columns = df.columns

    # âœ… SINGLE QUERY: Verifica todas as colunas de uma vez
    count_expressions = []
    for column in all_columns:
        count_expr = f"COUNT(*) FILTER (WHERE {column} IS NOT NULL) as {column}_count"
        count_expressions.append(count_expr)

    count_query = f"""
        SELECT {',\n            '.join(count_expressions)}
        FROM {prod_schema}.{demo_name}{table_suffix}
    """

    counts_df = spark.sql(count_query)
    counts_row = counts_df.collect()[0]

    accepted_columns = [col for i, col in enumerate(all_columns) if counts_row[i] > 0]
    return accepted_columns
```

**Vantagens:**
1. âœ… **Apenas 1 query** para todas as colunas
2. âœ… **1 full table scan** ao invÃ©s de N
3. âœ… Usa `COUNT(*) FILTER` nativo do Spark (otimizado)
4. âœ… **Custo fixo independente do nÃºmero de colunas**

---

## ğŸ“Š Benchmark Estimado

### CenÃ¡rio: Tabela com 1 bilhÃ£o de linhas, 50 colunas

| MÃ©trica | Original (N queries) | Otimizada (1 query) | Sampling (0.01) |
|---------|---------------------|---------------------|----------------|
| **Queries executadas** | 50 | 1 | 1 |
| **Linhas processadas** | 50B (50Ã—1B) | 1B | 10M (0.01Ã—1B) |
| **Tempo estimado** | ~500 segundos | ~10 segundos | ~0.1 segundo |
| **DBU consumido** | ~50 DBU | ~1 DBU | ~0.01 DBU |
| **Custo estimado** | $50 | $1 | $0.01 |
| **Economia** | Baseline | **98%** ğŸ’° | **99.98%** ğŸ’°ğŸ’°ğŸ’° |

*Valores aproximados para ilustraÃ§Ã£o. Custos reais dependem do cluster e regiÃ£o.*

---

## âš¡ RecomendaÃ§Ãµes Finais

1. **âœ… Use `freeport_module_template_optimized.py`** ao invÃ©s do original
2. **âœ… PadrÃ£o recomendado**: Single Query Optimization (jÃ¡ 98% mais barato)
3. **âš ï¸ Para tabelas gigantes**: Considere Sampling com `sample_fraction=0.01`
4. **ğŸ“Š Monitore custos**: Adicione logging para acompanhar execuÃ§Ãµes
5. **ğŸ§ª Teste primeiro**: Rode em subset pequeno antes de produÃ§Ã£o

---

## ğŸ”„ MigraÃ§Ã£o do CÃ³digo Existente

### Antes (âŒ CÃ³digo antigo)

```python
from freeport_module_template import freeport_module

deliverable = freeport_module(sandbox_schema, prod_schema, demo_name, "pro_insights")
# âŒ Usa N queries (caro!)
```

### Depois (âœ… CÃ³digo otimizado)

```python
from freeport_module_template_optimized import freeport_module

deliverable = freeport_module(sandbox_schema, prod_schema, demo_name, "pro_insights")
# âœ… Usa 1 query (98% mais barato!)
```

**MudanÃ§a mÃ­nima, economia mÃ¡xima!** ğŸ‰

---

## ğŸ“š ReferÃªncias

- **Arquivo original**: `freeport_module_template.py`
- **Arquivo otimizado**: `freeport_module_template_optimized.py`
- **Este guia**: `OPTIMIZATION_GUIDE.md`
- **Spark COUNT FILTER docs**: [Spark SQL Reference](https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html)
